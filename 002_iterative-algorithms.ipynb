{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Newton, Rhapson and a Calculator: Iterative Algorithms\n",
    "\n",
    "## Last Week in Babylon\n",
    "\n",
    "To begin this week's class, we will quickly revisit what we did last time. First we shall import the `math` module &mdash; always a good idea!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the previous lesson: we explored some of the basic tools to perform calculations in Python 3, and put them together to implement the Babylonian square root algorithm. The algorithm is a simple iterative process to estimate $r$, the square root of a given number $s=r^2$:\n",
    "\n",
    "- Take an initial guess for $r$, call it $x_0$. \n",
    "- Compute the update $x_{n+1}$ from the previous step $x_n$ using the rule:\n",
    "$$x_{n+1}=\\frac{x_n+\\frac{s}{x_n}}{2}.$$\n",
    "- Repeat until the guess is no longer changing.\n",
    "\n",
    "We tried a few different implementations last time, and eventually settled for:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might rememeber that the algorithm showed excellent convergence. For instance, $\\sqrt{\\pi}$ is computed in only 6 iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In writing the algorithm in Python we made use of several tools. We learnt how to define our own *functions*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also saw how to use *`for` loops* to implement iteration or repetition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, but not least, we began to use *`if` statements* in order to execute certain parts of the code conditionally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving a Fixed Point Equation\n",
    "\n",
    "Last week we set the task of solving an equation:\n",
    "$$\\cos(x)=x.$$\n",
    "This, which might seem like a completely new problem, is actually very similar to the Babylonian square root; a solution can be found by iteration!\n",
    "\n",
    "![calculator][logo]\n",
    "\n",
    "[logo]: https://upload.wikimedia.org/wikipedia/commons/thumb/2/2d/TI-84_Plus_graphing.jpg/151px-TI-84_Plus_graphing.jpg \"TI 84\"\n",
    "\n",
    "If you ever had a scientific calculator like the TI-84 I had in school, you might have tried to press the `cos` key repeatedly. It really did not matter what was on screen before; in just a few presses you would always arrive to the same number: `0.73908...` You may not have been aware of this then, but you were actually solving the equation $\\cos(x)=x$.\n",
    "\n",
    "Essentially, given the initial value $x_0$ (whichever value was on the screen at the beginning), you were computing the update:\n",
    "$$x_{n+1}=\\cos(x_n).$$\n",
    "A Babylonian-flavoured update indeed! We can implement a function which computes the solution as required:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps not as fast as the Babylonian root method, but the algorithm still finishes in less than 100 iterations!\n",
    "\n",
    "You can try changing the initial guess over and over and you should see that, once again, it does not really matter what we choose. Positive and negative numbers, large or small, this algorithm always converges to the same solution (the only solution, in fact) in about 90 iterations.\n",
    "\n",
    "Can all problems in life be solved via iterative algorithms? Unfortunately, no. While this method works well for the cosine equation, it will not work for a general equation\n",
    "$$f(x)=x.$$\n",
    "\n",
    "Many things can go wrong. First, and this may seem obvious to you, a crucially important point: the equation $f(x)=x$ may not have a solution! Even the best of algorithms will fail if we use it to tackle an impossible problem. Did you ever try pressing the `exp` key on your calculator instead? The equation\n",
    "$$\\exp(x)=x$$\n",
    "does not have real solutions, so the algorithm cannot possibly converge. Try to implement the algorithm; you should see the guess becoming exponentially large and causing an `OverflowError` in just a few iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, maybe there is a solution but the convergence is bad! The cosine equation just happened to yield good convergence, but many other equations will not. Try for instance\n",
    "$$\\tan(x)=x.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardless of how many iterations you set at the beginning, the for loop finishes long before the algorithm has converged. You may also try the sine equation,\n",
    "$$\\sin(x)=x,$$\n",
    "which is a closely related problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, bad convergence. It is not that the algorithm is not converging, or that it is converging to the wrong answer &mdash; the algorithm works! It is simply converging very slowly. We could talk about how choosing an initial guess closer to the solution (which is $x=0$, by the way) saves some time in the computation, but altogether this is not a good method to solve equations.\n",
    "\n",
    "## The name is Newton, Sir Isaac Newton\n",
    "\n",
    "Since we have seen a **bad** method for solving equations, we should also see a **good** one (a better one, at any rate). We are going to use the *Newton-Raphson method* (sometimes called simply Newton's method), due to Sir Isaac Newton <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/84/Isaac_Newton_signature_ws.svg/320px-Isaac_Newton_signature_ws.svg.png\"\n",
    "     alt=\"\"\n",
    "     style=\"display: inline; width: 100px;\" />\n",
    "     and Joseph Raphson <img src=\"https://upload.wikimedia.org/wikipedia/en/a/ac/Joseph_raphson_signature.jpg\"\n",
    "     alt=\"\"\n",
    "     style=\"display: inline; width: 150px;\" />.\n",
    "\n",
    "Say we want to solve a general equation in one variable,\n",
    "$$f(x)=0.$$\n",
    "As usual, we make a guess $x_0$ of the true solution $x^*$. Now, if life were easy, $f$ would be a linear function, $f(x)=mx+b$. In this case we know the explicit solution $x^*=-b/m$, but we can instead write the solution as a point which is one step away from the intial guess. Using the gradient of the linear graph and a quick drawing, it is easy to see\n",
    "$$x^*=x_0-\\frac{f(x_0)}{m},$$\n",
    "or indeed\n",
    "$$x^*=x_0-\\frac{f(x_0)}{f'(x_0)},$$\n",
    "since $f'(x)=m$ for any $x$.\n",
    "\n",
    "Of course life is not always easy, and functions are not always linear. However, this method still seems like an intuitive way of moving closer to the solution. The Babylonian within you is probably screaming the iteration $$x_{n+1}=x_n-\\frac{f(x_n)}{f'(x_n)}.$$\n",
    "This iteration is know as the *Newton-Raphson method*.\n",
    "\n",
    "### Solving a Polynomial\n",
    "Using Newton's method, solve:\n",
    "$$x^3-2x^2-11x+12=0.$$\n",
    "Which solution do you get? How many solutions are there? Can you control which solution you find?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Stopping Condition\n",
    "\n",
    "When do we stop the algorithm? So far we have done something very crude: we only stop the iteration when $x_{n+1}=x_n$, i.e. when the difference between two consecutive guesses is exactly zero. \n",
    "\n",
    "Thus far we have only considered *nice* problems. Iterations which converge well, algorithms that do not require a lot of processing power, and solutions which can be computed as precisely as a machine can go. This might not be the case later on in life. We might have, for instance, an iteration which requires an expensive computation to find each update &mdash; a few minutes or hours. In this case, we certainly cannot afford to wait for the difference of the iteration to become zero; we might have to accept an approximate solution. A very common alternative for such situations is to specify a *tolerance*, and stop the iteration whenever the difference goes below such tolerance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reformulate this in a nicer way using a `while` loop. A `while` loop is somehow a mixture of a `for` loop and an `if` statement. Take a look at the example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count=10;\n",
    "\n",
    "while count>0:\n",
    "    count=count-1\n",
    "    print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike `for` loops, which run for a fixed number of iterations unless they are stopped, `while` loops run as long as the condition is `True`. The loop above counts backwards as long as the count is a positive number, but it stops as soon as it reaches zero.\n",
    "\n",
    "Such loops can be useful for algorithms such as our `newton` function since we no longer have to choose a maximum number of iterations. However, `while` loops can be dangerous if we are not sure that the condition will ever become `False`. In the example, replacing `count=count-1` by `count=count+1` would create an infinite loop, and the program would never stop running. In general, it can be quite hard to determine whether a program finishes or runs for ever &mdash; this is known as the [Halting Problem](https://en.wikipedia.org/wiki/Halting_problem)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VI. Machine Learning Part I: The Tools\n",
    "\n",
    "## Regression\n",
    "\n",
    "Regression is a series of statistical tools to find the equation of a function which best fits some scattered data. We will generate data with an underlying trend, make a guess of this trend, and devise an iteration to improve this guess.\n",
    "\n",
    "#### A Linear Trend\n",
    "\n",
    "We will start with the simplest case in the book: a linear pattern. This is a chance for us to become acquainted with some of the fundamental working part of a neural network, which we will implement shortly.\n",
    "\n",
    "First we import some tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import rand\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin by encoding the equation of a line\n",
    "$$y=mx+b,$$\n",
    "as a python function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For validation, rather than using real data, we will generate our own. First we will fix the parameters of the linear equation; we will use capital letters, $M$ and $B$, to distinguish the *real* parameters from the *estimated* ones later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we generate the data. First, we make an array of $N$ values of the $x$-coordinate, $X_i$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first define $L_i=mX_i+b$, the $y$-values which truly satisfy the linear relation. Then, we define noisy $y$-values by $Y_i=L_i+\\sigma Z_i$, where $Z_i\\sim U(-1,1)$, a uniform random variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting The Data\n",
    "\n",
    "We now have pairs $(X_i, Y_i)$ which represent data with a noisy linear trend. We can plot the pairs over the linear equation to visualise the trend with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scatter plot\n",
    "plt.scatter(X, Y, c='b', marker='.')\n",
    "\n",
    "#Line plot\n",
    "t = np.linspace(0,1,100)\n",
    "plt.plot(t, linear(t, M, B), c='g')\n",
    "\n",
    "#Labels\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend([\"Linear trend\", \"Noisy data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first line simply makes a scatter plot of the $(X_i, Y_i)$ pairs, in blue. To plot the linear trend we create an array of ordered $x$-coordinates, `t`, using the `np.linspace` function; then we plot the `linear` function applied to `t` to present a \"continuous\" line.\n",
    "\n",
    "#### Guessing the Trend\n",
    "\n",
    "The data is noisy as intended, but the linear trend is clear. Suppose now we didn't know the values of $M$ and $B$. How could we approximate them? What we will do is, as usual, begin with a guess and come up with a way of improving it iteratively. First we guess approximate values of $M$ and $B$, and we call them $m$ and $b$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now apply the linear equation with parameters $m$ and $b$ to $X_i$ to obtain our guesses for the $y$-coordinate, $y_i$. If our guess is good, the values of $y_i$ will be *close* to the real ordinates $Y_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing the Trends\n",
    "\n",
    "We could show these values as a scatter plot, but the comparison is easier if we simply add the line with coefficients $m$ and $b$ to our previous plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Previous plots\n",
    "plt.scatter(X, Y, marker='.')\n",
    "plt.plot(t, linear(t, M, B), 'g')\n",
    "\n",
    "#New plots\n",
    "plt.plot(t, linear(t, m, b), 'r')\n",
    "\n",
    "#Labels\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend([\"Linear trend\", \"Approximate trend\", \"Noisy data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our initial guess for the trend is plotted in red, whereas the real trend is plotted in green. If you picked $m$ and $b$ somewhat randomly, the guess is probably not a very good one. We will now work to improve this.\n",
    "\n",
    "#### The Loss Function\n",
    "\n",
    "The first thing to do is to quantify the quality of the approximation. We need a numerical value, a **score**, which lets us compare different guesses; this is sometimes called a *cost function*, an *objective function*, or a **loss function**.\n",
    "\n",
    "A typical loss function for trend estimation is the [*mean squared error* (MSE)](https://en.wikipedia.org/wiki/Mean_squared_error):\n",
    "$$\n",
    "\\mathcal{J} = \\frac{1}{N}\\sum_{i=1}^{N}(Y_i-y_i)^2 = \\frac{1}{N}\\sum_{i=1}^{N}(Y_i-mX_i-b)^2.\n",
    "$$\n",
    "This is a nice choice of loss function for a number of reasons. For instance, squaring the error makes small deviations even smaller whilst enlarging deviations greater than $1$. In a way, this loss function puts the emphasis on large erros and \"forgives\" the smaller deviations; it does not matter if the guess is inexact as long as no point is missed by a large margin. This is important because the noisy data is *not* exactly linear, even though it is *close* to linear.\n",
    "\n",
    "We can define the `loss` function succinctly with a list comprehension and test the loss of the current guess:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minima of the Loss Function\n",
    "\n",
    "Now that we have a guess and its corresponding score, we should try to improve it. Another of the nice properties of the MSE loss function is that it is differentiable with respect to the parameters $m$ and $b$. We can compute the derivatives explicitly. By recalling the cost\n",
    "\\begin{align}\n",
    "\\mathcal{J} = \\frac{1}{N}\\sum_{i=1}^{N}(Y_i-y_i)^2 = \\frac{1}{N}\\sum_{i=1}^{N}(Y_i-mX_i-b)^2,\n",
    "\\end{align}\n",
    "we obtain\n",
    "\\begin{align}\n",
    "\\frac{\\partial\\mathcal{J}}{\\partial m}\n",
    "& = \\frac{-2}{N}\\sum_{i=1}^{N}(Y_i-mX_i-b)X_i\n",
    "= \\frac{-2}{N}\\sum_{i=1}^{N}(Y_i-y_i)X_i,\n",
    "\\\\\\frac{\\partial\\mathcal{J}}{\\partial b}\n",
    "& = \\frac{-2}{N}\\sum_{i=1}^{N}(Y_i-mX_i-b)\n",
    "= \\frac{-2}{N}\\sum_{i=1}^{N}(Y_i-y_i).\n",
    "\\end{align}\n",
    "Both of these derivatives should be zero once we find the minimisers of the loss function, i.e. the values of $m$ and $b$ which yield a minimum of the error. We will not consider the issue of several minima for now.\n",
    "\n",
    "We can implement functions for the derivatives and check their current value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent\n",
    "\n",
    "The current derivatives are far from being zero, which means the loss is far from being minimal. We can use gradient descent to update our guesses.\n",
    "\n",
    "In general, gradient descent is a method for finding a minimum of a function. If we are trying to minimise a function $f:\\mathbb{R}^n\\rightarrow \\mathbb{R}$, we begin with an initial guess $x_0\\in\\mathbb{R}^n$ and compute the gradient of $f$ at that point, $\\nabla f(x_0)$. We then update our guess by\n",
    "$$\n",
    "x_1 = x_0 - s_0\\nabla f(x_0)\n",
    "$$\n",
    "for some positive constant $s_0$. In general, the iteration is\n",
    "$$\n",
    "x_{n+1} = x_n - s_n\\nabla f(x_n).\n",
    "$$\n",
    "Note that if $f$ takes only a real argument, the gradient is simply the derivative:\n",
    "$$\n",
    "x_{n+1} = x_n - s_n\\frac{\\mathrm{d}f}{\\mathrm{d}x}(x_n).\n",
    "$$\n",
    "The $s_n$ have to be chosen small enough to ensure that $f(x_{n+1})\\leq f(x_{n})$. A typical approach is to fix $s$ and begin the descent checking that $f(x_{n+1})\\leq f(x_{n})$ at every step. As soon as $f(x_{n+1})>f(x_{n})$, the current $s$ is rejected and a new, smaller constant is chosen.\n",
    "\n",
    "In our case, thinking of $\\mathcal{J}$ as a function of $m$ and $b$, the gradient is\n",
    "$$\n",
    "\\nabla\\mathcal{J}=\\left(\\frac{\\partial\\mathcal{J}}{\\partial m},\\frac{\\partial\\mathcal{J}}{\\partial b}\\right).\n",
    "$$\n",
    "The iteration will look like\n",
    "$$\n",
    "\\left(m_{n+1},b_{n+1}\\right)\n",
    "= \\left(m_{n},b_{n}\\right)\n",
    "- s_n\\left(\\frac{\\partial\\mathcal{J}}{\\partial m},\\frac{\\partial\\mathcal{J}}{\\partial b}\\right)\n",
    ",\n",
    "$$\n",
    "which can be done separately for $m$ and for $b$ as\n",
    "\\begin{align}\n",
    "m_{n+1} &= m_{n}- s_n\\frac{\\partial\\mathcal{J}}{\\partial m},\n",
    "\\\\b_{n+1} &= b_{n}- s_n\\frac{\\partial\\mathcal{J}}{\\partial b}.\n",
    "\\end{align}\n",
    "\n",
    "Like every iteration, we will need a stopping criterion. We will select a tolerance $\\varepsilon$ and stop whenever $\\left|\\frac{\\partial\\mathcal{J}}{\\partial m}\\right|<\\varepsilon$ and $\\left|\\frac{\\partial\\mathcal{J}}{\\partial b}\\right|<\\varepsilon$.\n",
    "\n",
    "We will now implement the descent algorithm, storing each value of the loss in a list `lossList` to plot it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have convergence! The optimal values of $m$ and $b$ we recover might be a little different from $M$ and $B$, both because we only have an approximation, but also because the *true* linear trend of the data might be slightly different from the trend given by $M$ and $B$ since we only have a finite number of samples.\n",
    "\n",
    "We can plot the estimated trend to see whether or not it captures the trend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, Y, marker='.')\n",
    "plt.plot(t, linear(t, M, B), 'g')\n",
    "plt.plot(t, linear(t, m, b), 'r')\n",
    "\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend([\"Linear trend\", \"Approximate trend\", \"Noisy data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the `lossList` over the number of iterations to see how it has decreased:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(range(0,len(lossList)),lossList)\n",
    "\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Linear Regression\n",
    "\n",
    "The problem we have just considered, often called the [Linear Least Squares problem](https://en.wikipedia.org/wiki/Linear_least_squares), actually has an exact solution, easy to compute but not so easy to justify. However, the gradient descent method we have used is more general, and will let us deal with non-linear data as well.\n",
    "\n",
    "This time we are going to assume the data has a quadratic trend, given by\n",
    "$$\n",
    "y = ax^2+bx+c.\n",
    "$$\n",
    "To generate some validation data, we will define the quadratic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose the quadratic $10x^2-10x+5$, simply because it has a nice shape on the interval $[0,1]$. We define the true parameters, $A, B, C$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we generate random $x$-coordinates $X_i$,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we also generate the true quadratic values $L_i=AX_i^2+BX_i+C$ as well as the noisy values $Y_i=L_i+\\sigma Z_i$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We proceed to plot the trend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scatter plot\n",
    "plt.scatter(X, Y, c='b', marker='.')\n",
    "\n",
    "#Quadratic plot\n",
    "t = np.linspace(0,1,100)\n",
    "plt.plot(t, quadratic(t, A, B, C), c='g')\n",
    "\n",
    "#Labels\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend([\"Quadratic trend\", \"Noisy data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to make a guess, define the loss function, compute its derivatives and descend its gradient.\n",
    " \n",
    "We can initialise the approximate parameters to anything in practice. As we saw before, the quadratic error causes the loss function to decrease rapidly at the beginning; the parameters will rapidly become *close* to the real ones, even if full convergence will take longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again we can evaluate the $y$-coordinates corresponding to our current guess and plot the guessed cubic trend to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, Y, marker='.')\n",
    "plt.plot(t, quadratic(t, A, B, C), 'g')\n",
    "plt.plot(t, quadratic(t, a, b, c), 'r')\n",
    "\n",
    "#Labels\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend([\"Quadratic trend\", \"Approximate trend\", \"Noisy data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will again define a MSE loss function,\n",
    "$$\n",
    "\\mathcal{J} = \\frac{1}{N}\\sum_{i=1}^{N}(Y_i-y_i)^2 = \\frac{1}{N}\\sum_{i=1}^{N}\\left[Y_i-(aX_i^2+bX_i+c)\\right]^2,\n",
    "$$\n",
    "and compute its derivatives with respect to $a, b$ and $c$:\n",
    "\\begin{align}\n",
    "\\frac{\\partial\\mathcal{J}}{\\partial a}\n",
    "& = \\frac{-2}{N}\\sum_{i=1}^{N}\\left[Y_i-(aX_i^2+bX_i+c)\\right]X_i^2\n",
    "= \\frac{-2}{N}\\sum_{i=1}^{N}(Y_i-y_i)X_i^2\n",
    ",\\\\\\frac{\\partial\\mathcal{J}}{\\partial b}\n",
    "& = \\frac{-2}{N}\\sum_{i=1}^{N}\\left[Y_i-(aX_i^2+bX_i+c)\\right]X_i\n",
    "= \\frac{-2}{N}\\sum_{i=1}^{N}(Y_i-y_i)X_i\n",
    ",\\\\\\frac{\\partial\\mathcal{J}}{\\partial c}\n",
    "& = \\frac{-2}{N}\\sum_{i=1}^{N}\\left[Y_i-(aX_i^2+bX_i+c)\\right]\n",
    "= \\frac{-2}{N}\\sum_{i=1}^{N}(Y_i-y_i)\n",
    ".\n",
    "\\end{align}\n",
    "\n",
    "We can implement these &mdash; in fact we can recycle some of the code from the linear regression section!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient descent algorithm for the problem looks very similar to the previous one:\n",
    "\\begin{align}\n",
    "a_{n+1} &= a_{n}- s_n\\frac{\\partial\\mathcal{J}}{\\partial a},\n",
    "\\\\b_{n+1} &= b_{n}- s_n\\frac{\\partial\\mathcal{J}}{\\partial b},\n",
    "\\\\c_{n+1} &= c_{n}- s_n\\frac{\\partial\\mathcal{J}}{\\partial c},\n",
    "\\\\d_{n+1} &= d_{n}- s_n\\frac{\\partial\\mathcal{J}}{\\partial d}.\n",
    "\\end{align}\n",
    "\n",
    "All that remains now is the gradient descent loop, which will also look very similar to the previous loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convergence is significantly slower than it was in the linear case. In the end, however, we obtain a nice fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, Y, marker='.')\n",
    "plt.plot(t, quadratic(t, A, B, C), 'g')\n",
    "plt.plot(t, quadratic(t, a, b, c), 'r')\n",
    "\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend([\"Quadratic trend\", \"Approximate trend\", \"Noisy data\"])\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.semilogy(range(0,len(lossList)),lossList)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sometimes, Gradients Won't Do\n",
    "\n",
    "### The Cubic Polynomial\n",
    "\n",
    "The blocks below implement the gradient descent for the cubic fitting problem, \n",
    "$$\n",
    "y = ax^3+bx^2+cx+d,\n",
    "$$\n",
    "and proceed just as before. Run them and look at the results of the descent. Here are the functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "def cubic(x, a, b, c, d): return a*x**3 + b*x**2 + c*x + d\n",
    "def loss(N, y, X, Y): return sum([(Y[k]-y[k])**2 for k in range(0, N)])/N\n",
    "def aDerivative(N, y, X, Y): return -2*sum([(Y[k]-y[k])*X[k]**3 for k in range(0, N)])/N\n",
    "def bDerivative(N, y, X, Y): return -2*sum([(Y[k]-y[k])*X[k]**2 for k in range(0, N)])/N\n",
    "def cDerivative(N, y, X, Y): return -2*sum([(Y[k]-y[k])*X[k] for k in range(0, N)])/N\n",
    "def dDerivative(N, y, X, Y): return -2*sum([(Y[k]-y[k]) for k in range(0, N)])/N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "A = 10\n",
    "B = -15\n",
    "C = 5\n",
    "D = 2\n",
    "\n",
    "N = 200\n",
    "X = rand(N)\n",
    "\n",
    "L = cubic(X, A, B, C, D)\n",
    "\n",
    "sigma = 0.3\n",
    "Y = L + (rand(N)*2-1)*sigma\n",
    "\n",
    "a = (rand()*2-1)*10\n",
    "b = (rand()*2-1)*10\n",
    "c = (rand()*2-1)*10\n",
    "d = (rand()*2-1)*10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have chosen initial guesses which are very close to the real values! Here is the plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting\n",
    "\n",
    "plt.scatter(X, Y, marker='.')\n",
    "plt.plot(t, cubic(t, A, B, C, D), 'g')\n",
    "plt.plot(t, cubic(t, a, b, c, d), 'r')\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend([\"Cubic trend\", \"Approximate trend\", \"Noisy data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, here is the main loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (rand()*2-1)*10\n",
    "b = (rand()*2-1)*10\n",
    "c = (rand()*2-1)*10\n",
    "d = (rand()*2-1)*10\n",
    "\n",
    "y = cubic(X, a, b, c, d)\n",
    "\n",
    "s = 1000\n",
    "tol = 1.0e-8\n",
    "\n",
    "lossCurrent = loss(N, y, X, Y)\n",
    "lossList = [lossCurrent]\n",
    "\n",
    "for k in range(0, 10000):\n",
    "    aDer = aDerivative(N, y, X, Y)\n",
    "    bDer = bDerivative(N, y, X, Y)\n",
    "    cDer = cDerivative(N, y, X, Y)\n",
    "    dDer = dDerivative(N, y, X, Y)\n",
    "    \n",
    "    aDerAbs = abs(aDer)\n",
    "    bDerAbs = abs(bDer)\n",
    "    cDerAbs = abs(cDer)\n",
    "    dDerAbs = abs(dDer)\n",
    "    \n",
    "    aNew = a - s*aDer\n",
    "    bNew = b - s*bDer\n",
    "    cNew = c - s*cDer\n",
    "    dNew = d - s*dDer\n",
    "    \n",
    "    yNew = cubic(X, aNew, bNew, cNew, dNew)\n",
    "\n",
    "    lossNew = loss(N, yNew, X, Y)\n",
    "\n",
    "    if lossNew > lossCurrent:\n",
    "        s = s/2\n",
    "        print(f\"k = {k}, descent failed. New step = {s}\")\n",
    "    else:\n",
    "        a = aNew\n",
    "        b = bNew\n",
    "        c = cNew\n",
    "        d = dNew\n",
    "        y = yNew\n",
    "        \n",
    "        lossCurrent = lossNew\n",
    "        lossList.append(lossCurrent)\n",
    "        print(f\"k = {k}. Loss = {lossNew}.\")\n",
    "    \n",
    "    if aDerAbs<tol and bDerAbs<tol and cDerAbs<tol and dDerAbs<tol:\n",
    "        break\n",
    "\n",
    "print(f\"\\n True values: A = {A}, B = {B}, C = {C}, D = {D}.\")\n",
    "print(f\"\\n Approximate values: a = {a}, b = {b}, c = {c}, d = {d}.\")\n",
    "\n",
    "plt.scatter(X, Y, marker='.')\n",
    "plt.plot(t, cubic(t, A, B, C, D), 'g')\n",
    "plt.plot(t, cubic(t, a, b, c, d), 'r')\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend([\"Cubic trend\", \"Approximate trend\", \"Noisy data\"])\n",
    "\n",
    "plt.figure()\n",
    "plt.semilogy(range(0,len(lossList)),lossList)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What went wrong? Well... nothing. The algorithm has run for 10000 iterations and simply has not converged yet. The choice of parameters have improved, the guess is *close* to the original trend, the loss has decreased over time, but the algorithm simply isn't done yet because the optimisation in four parameters is slow. **Gradient descent is slow**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Barzilai-Borwein Update\n",
    "\n",
    "One of the reasons the previous algorithm was very slow is that the step $s_n$ was not set in a very thoughtful way; once it decreased, it was not allowed to increase again. We will use this type of gradient descent to implement a neural network in the future, but you should know that there are more sophisticated optimisation methods out there. You cannot learn them all in one lesson, but here we will explore some code examples.\n",
    "\n",
    "The [Barzilai-Borwein update](https://doi.org/10.1093/imanum/8.1.141) is a **very** clever choice of the steps $s_n$ which greatly accelerates the gradient descent. If we are trying to minimise a function $f:\\mathbb{R}^n\\rightarrow \\mathbb{R}$ with the gradient descent iteration\n",
    "$$\n",
    "x_{n+1} = x_n - s_n\\nabla f(x_n),\n",
    "$$\n",
    "we define the Barzilai-Borwein update by either\n",
    "$$\n",
    "s_n=\\frac{\n",
    "\\langle\n",
    "x_{n}-x_{n-1}\n",
    ",\n",
    "\\nabla f(x_{n})-\\nabla f(x_{n-1})\n",
    "\\rangle\n",
    "}{\n",
    "\\|\\nabla f(x_{n})-\\nabla f(x_{n-1})\\|^2\n",
    "}\n",
    "$$\n",
    "or\n",
    "$$\n",
    "s_n=\\frac{\n",
    "\\|x_{n}-x_{n-1}\\|^2\n",
    "}{\n",
    "\\langle\n",
    "x_{n}-x_{n-1}\n",
    ",\n",
    "\\nabla f(x_{n})-\\nabla f(x_{n-1})\n",
    "\\rangle\n",
    "}.\n",
    "$$\n",
    "\n",
    "To find out why this is a good choice of the step you can go to the [original paper](https://doi.org/10.1093/imanum/8.1.141). However, we can verify this numerically. The code below implements the gradient descent optimisation of the same cubic problem we attempted before, only this time the step $s_n$ is chosen according to this new rule. Try to run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (rand()*2-1)*10\n",
    "b = (rand()*2-1)*10\n",
    "c = (rand()*2-1)*10\n",
    "d = (rand()*2-1)*10\n",
    "\n",
    "y = cubic(X, a, b, c, d)\n",
    "\n",
    "s = 0.01\n",
    "tol = 1.0e-8\n",
    "\n",
    "lossCurrent = loss(N, y, X, Y)\n",
    "lossList = [lossCurrent]\n",
    "\n",
    "for k in range(0, 1000):\n",
    "    aDer = aDerivative(N, y, X, Y)\n",
    "    bDer = bDerivative(N, y, X, Y)\n",
    "    cDer = cDerivative(N, y, X, Y)\n",
    "    dDer = dDerivative(N, y, X, Y)\n",
    "    \n",
    "    aDerAbs = abs(aDer)\n",
    "    bDerAbs = abs(bDer)\n",
    "    cDerAbs = abs(cDer)\n",
    "    dDerAbs = abs(dDer)\n",
    "    \n",
    "    # Barzilai-Borwein\n",
    "    if k>0:\n",
    "        num = (a-aOld)**2+(b-bOld)**2+(c-cOld)**2+(d-dOld)**2\n",
    "        den = (a-aOld)*(aDer-aDerOld)+(b-bOld)*(bDer-bDerOld)+(c-cOld)*(cDer-cDerOld)+(d-dOld)*(dDer-dDerOld)\n",
    "        s = num/den\n",
    "    \n",
    "    aNew = a - s*aDer\n",
    "    bNew = b - s*bDer\n",
    "    cNew = c - s*cDer\n",
    "    dNew = d - s*dDer\n",
    "    \n",
    "    yNew = cubic(X, aNew, bNew, cNew, dNew)\n",
    "\n",
    "    lossNew = loss(N, yNew, X, Y)\n",
    "\n",
    "    aOld = a\n",
    "    bOld = b\n",
    "    cOld = c\n",
    "    dOld = d\n",
    "    \n",
    "    a = aNew\n",
    "    b = bNew\n",
    "    c = cNew\n",
    "    d = dNew\n",
    "    y = yNew\n",
    "        \n",
    "    aDerOld = aDer\n",
    "    bDerOld = bDer\n",
    "    cDerOld = cDer\n",
    "    dDerOld = dDer\n",
    "    \n",
    "    lossCurrent = lossNew\n",
    "    lossList.append(lossCurrent)\n",
    "    print(f\"k = {k}. Loss = {lossNew}.\")\n",
    "    \n",
    "    if aDerAbs<tol and bDerAbs<tol and cDerAbs<tol and dDerAbs<tol:\n",
    "        break\n",
    "\n",
    "print(f\"\\n True values: A = {A}, B = {B}, C = {C}, D = {D}.\")\n",
    "print(f\"\\n Approximate values: a = {a}, b = {b}, c = {c}, d = {d}.\")\n",
    "\n",
    "plt.scatter(X, Y, marker='.')\n",
    "plt.plot(t, cubic(t, A, B, C, D), 'g')\n",
    "plt.plot(t, cubic(t, a, b, c, d), 'r')\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend([\"Cubic trend\", \"Approximate trend\", \"Noisy data\"])\n",
    "\n",
    "plt.figure()\n",
    "plt.semilogy(range(0,len(lossList)),lossList)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beautiful convergence in just a few iterations!\n",
    "\n",
    "Notice that the loss function does not decrease monotonically. This method is clearly doing something more complicated than simple descent. Remember this as a nice trick for the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
